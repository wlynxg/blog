<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="新闻分类：多分类问题 简介 本例出自《Python 深度学习》，自己做了一个简单的总结归纳。\n完整代码请参考：[https://github.com/fchollet/deep-learning-with-python-notebooks](\n数据预处理 graph LR A[原始新闻内容] --> |关键词分割|C[建立关键词索引] C --> |将关键词转索引|D[原始评论转向量] D --> 列表编码为二进制矩阵 graph LR 原始评论标签 --> one-hot编码将标签向量化 训练模型 graph LR A1(第一层:16个输出单元) --> A[构建模型] A2(第二层:16个输出单元) --> A A3(第三层:1个输出单元) --> A A ==> B[编译模型] B1(配置优化器和损失函数) --> B B ==> C[加入验证集训练模型] C ==> D[绘制图表观察模型最佳参数] D1[欠拟合与过拟合之间] --> D D ==> E[选择最佳参数训练模型] E ==> F[在新数据上使用模型] 代码 加载数据集 注意：第一次运行会下载数据集，速度较慢。\n1 2 3 from keras.datasets import reuters (train_data, train_label), (test_data, test_label) = reuters.load_data(num_words=10000) 将向量还原为原始新闻（非必须） 执行这一步只是为了更直观的了解别人是怎么处理数据的。\n"><title>新闻分类：多分类问题</title>
<link rel=canonical href=https://wlynxg.github.io/blog/p/%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/><link rel=stylesheet href=/blog/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="新闻分类：多分类问题"><meta property='og:description' content="新闻分类：多分类问题 简介 本例出自《Python 深度学习》，自己做了一个简单的总结归纳。\n完整代码请参考：[https://github.com/fchollet/deep-learning-with-python-notebooks](\n数据预处理 graph LR A[原始新闻内容] --> |关键词分割|C[建立关键词索引] C --> |将关键词转索引|D[原始评论转向量] D --> 列表编码为二进制矩阵 graph LR 原始评论标签 --> one-hot编码将标签向量化 训练模型 graph LR A1(第一层:16个输出单元) --> A[构建模型] A2(第二层:16个输出单元) --> A A3(第三层:1个输出单元) --> A A ==> B[编译模型] B1(配置优化器和损失函数) --> B B ==> C[加入验证集训练模型] C ==> D[绘制图表观察模型最佳参数] D1[欠拟合与过拟合之间] --> D D ==> E[选择最佳参数训练模型] E ==> F[在新数据上使用模型] 代码 加载数据集 注意：第一次运行会下载数据集，速度较慢。\n1 2 3 from keras.datasets import reuters (train_data, train_label), (test_data, test_label) = reuters.load_data(num_words=10000) 将向量还原为原始新闻（非必须） 执行这一步只是为了更直观的了解别人是怎么处理数据的。\n"><meta property='og:url' content='https://wlynxg.github.io/blog/p/%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/'><meta property='og:site_name' content="Wlynxg's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2025-08-22T17:34:52+08:00'><meta property='article:modified_time' content='2025-08-22T17:34:52+08:00'><meta name=twitter:title content="新闻分类：多分类问题"><meta name=twitter:description content="新闻分类：多分类问题 简介 本例出自《Python 深度学习》，自己做了一个简单的总结归纳。\n完整代码请参考：[https://github.com/fchollet/deep-learning-with-python-notebooks](\n数据预处理 graph LR A[原始新闻内容] --> |关键词分割|C[建立关键词索引] C --> |将关键词转索引|D[原始评论转向量] D --> 列表编码为二进制矩阵 graph LR 原始评论标签 --> one-hot编码将标签向量化 训练模型 graph LR A1(第一层:16个输出单元) --> A[构建模型] A2(第二层:16个输出单元) --> A A3(第三层:1个输出单元) --> A A ==> B[编译模型] B1(配置优化器和损失函数) --> B B ==> C[加入验证集训练模型] C ==> D[绘制图表观察模型最佳参数] D1[欠拟合与过拟合之间] --> D D ==> E[选择最佳参数训练模型] E ==> F[在新数据上使用模型] 代码 加载数据集 注意：第一次运行会下载数据集，速度较慢。\n1 2 3 from keras.datasets import reuters (train_data, train_label), (test_data, test_label) = reuters.load_data(num_words=10000) 将向量还原为原始新闻（非必须） 执行这一步只是为了更直观的了解别人是怎么处理数据的。\n"><link rel="shortcut icon" href=/blog/favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/file_hu6889846174828548036.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🐂</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>Wlynxg's Blog</a></h1><h2 class=site-description>The harder, ther luckier!</h2></div></header><ol class=menu-social><li><a href=https://github.com/wlynxg target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=mailto:liuauthor@foxmail.com target=_blank title=Mail rel=me><svg t="1730733819354" class="icon" viewBox="0 0 1024 1024" p-id="3570" width="24" height="24"><path d="M942.08 200.704s-1.024.0.0.0c-1.024-2.048-1.024-4.096-2.048-6.144-1.024-1.024-2.048-3.072-3.072-4.096l-1.024-1.024-1.024-1.024c-1.024-1.024-2.048-2.048-4.096-2.048h-1.024c-2.048-1.024-3.072-1.024-5.12-1.024h-1.024C788.48 163.84 649.216 153.6 512 153.6s-276.48 10.24-412.672 30.72h-1.024c-2.048.0-3.072 1.024-5.12 1.024H92.16c-1.024 1.024-3.072 1.024-4.096 2.048l-1.024 1.024-1.024 1.024c-1.024 1.024-2.048 2.048-3.072 4.096v1.024c-1.024 2.048-1.024 3.072-2.048 5.12C61.44 304.128 51.2 408.576 51.2 512s10.24 207.872 30.72 311.296c2.048 8.192 8.192 15.36 17.408 16.384C235.52 860.16 374.784 870.4 512 870.4s276.48-10.24 412.672-30.72c8.192-1.024 15.36-8.192 17.408-16.384C962.56 719.872 972.8 615.424 972.8 512s-10.24-207.872-30.72-311.296zm-60.416 19.456c-44.032 50.176-96.256 97.28-156.672 142.336C654.336 415.744 582.656 457.728 512 489.472c-77.824-36.864-148.48-78.848-212.992-126.976-56.32-41.984-108.544-90.112-156.672-142.336 122.88-17.408 246.784-25.6 369.664-25.6s246.784 8.192 369.664 25.6zm22.528 580.608C774.144 820.224 642.048 829.44 512 829.44s-262.144-9.216-392.192-28.672C101.376 705.536 92.16 608.256 92.16 512c0-87.04 8.192-175.104 22.528-262.144 48.128 53.248 102.4 102.4 159.744 145.408 69.632 52.224 144.384 96.256 229.376 135.168 3.072 1.024 6.144 2.048 8.192 2.048 3.072.0 5.12-1.024 8.192-2.048 76.8-32.768 153.6-78.848 229.376-135.168 60.416-46.08 114.688-94.208 159.744-144.384C924.672 337.92 931.84 424.96 931.84 512c0 96.256-9.216 193.536-27.648 288.768z" p-id="3571" fill="#707070"/></svg></a></li><li><a href=/blog/index.xml target=_blank title=RSS rel=me><svg t="1730718821658" class="icon" viewBox="0 0 1024 1024" p-id="21619" width="24" height="24"><path d="M554.666667 896C529.066667 896 512 878.933333 512 853.333333 512 665.6 358.4 512 170.666667 512 145.066667 512 128 494.933333 128 469.333333s17.066667-42.666667 42.666667-42.666666c234.666667.0 426.666667 192 426.666666 426.666666.0 25.6-17.066667 42.666667-42.666666 42.666667z" p-id="21620" fill="#707070"/><path d="M853.333333 896c-25.6.0-42.666667-17.066667-42.666666-42.666667.0-354.133333-285.866667-640-640-640-25.6.0-42.666667-17.066667-42.666667-42.666666S145.066667 128 170.666667 128C571.733334 128 896 452.266667 896 853.333333 896 878.933333 878.933333 896 853.333333 896z" p-id="21621" fill="#707070"/><path d="M213.333333 810.666667m-85.333333.0a85.333333 85.333333.0 10170.666667.0 85.333333 85.333333.0 10-170.666667.0z" p-id="21622" fill="#707070"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li><a href=/blog/plan/><svg t="1730733517435" class="icon" viewBox="0 0 1024 1024" p-id="1581" width="24" height="24"><path d="M874.24 1013.76H143.36a94.72 94.72.0 01-94.72-94.72V188.16a94.72 94.72.0 0194.72-94.72h730.88a95.36 95.36.0 0194.72 94.72v730.88a94.72 94.72.0 01-94.72 94.72zM143.36 156.8a31.36 31.36.0 00-32 31.36v730.88a32 32 0 0032 32h730.88a32 32 0 0031.36-32V188.16a31.36 31.36.0 00-31.36-31.36z" fill="#707070" p-id="1582"/><path d="M926.08 399.36h-832a32 32 0 010-64h832a32 32 0 110 64zM339.84 273.28a31.36 31.36.0 01-31.36-32V39.04a31.36 31.36.0 0131.36-32 32 32 0 0132 32v202.24a32 32 0 01-32 32zm337.28.0a31.36 31.36.0 01-31.36-32V39.04a31.36 31.36.0 1164 0v202.24a31.36 31.36.0 01-32.64 32zM444.16 841.6a33.92 33.92.0 01-23.04-9.6l-128-138.24a32 32 0 1146.08-43.52L448 768l280.32-279.68a30.72 30.72.0 0144.8.0 31.36 31.36.0 010 44.16L466.56 832a32 32 0 01-22.4 9.6z" fill="#707070" p-id="1583"/></svg>
<span>Plan</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#简介>简介</a><ol><li><a href=#数据预处理>数据预处理</a></li><li><a href=#训练模型>训练模型</a></li></ol></li><li><a href=#代码>代码</a><ol><li><a href=#加载数据集>加载数据集</a></li><li><a href=#将向量还原为原始新闻非必须>将向量还原为原始新闻（非必须）</a></li><li><a href=#将数据向量化>将数据向量化</a></li><li><a href=#使用-one-hot-编码将标签向量化>使用 one-hot 编码将标签向量化</a></li><li><a href=#构建模型>构建模型</a></li><li><a href=#验证模型>验证模型</a></li><li><a href=#绘制训练情况图表>绘制训练情况图表</a></li><li><a href=#重新训练模型>重新训练模型</a></li><li><a href=#绘制图表观察重新训练的模型各项指标-->绘制图表观察重新训练的模型各项指标 -</a></li></ol></li><li><a href=#小结>小结</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/ style=background-color:#7fc7d9;color:#fff>数据分析</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/>新闻分类：多分类问题</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Aug 22, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 3 分钟</time></div></footer></div></header><section class=article-content><h1 id=新闻分类多分类问题>新闻分类：多分类问题</h1><h2 id=简介>简介</h2><blockquote><p>本例出自《Python 深度学习》，自己做了一个简单的总结归纳。</p><p>完整代码请参考：[https://github.com/fchollet/deep-learning-with-python-notebooks](</p></blockquote><h3 id=数据预处理>数据预处理</h3><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR
	A[原始新闻内容] --&gt; |关键词分割|C[建立关键词索引]
	C --&gt; |将关键词转索引|D[原始评论转向量]
	D --&gt; 列表编码为二进制矩阵
</code></pre><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR
	原始评论标签 --&gt; one-hot编码将标签向量化
</code></pre><h3 id=训练模型>训练模型</h3><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR
	A1(第一层:16个输出单元) --&gt; A[构建模型]
	A2(第二层:16个输出单元) --&gt; A
	A3(第三层:1个输出单元) --&gt; A
	A ==&gt; B[编译模型]
	B1(配置优化器和损失函数) --&gt; B
	B ==&gt; C[加入验证集训练模型]
	C ==&gt; D[绘制图表观察模型最佳参数]
	D1[欠拟合与过拟合之间] --&gt; D
	D ==&gt; E[选择最佳参数训练模型]
	E ==&gt; F[在新数据上使用模型]
</code></pre><h2 id=代码>代码</h2><h3 id=加载数据集>加载数据集</h3><p>注意：第一次运行会下载数据集，速度较慢。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.datasets</span> <span class=kn>import</span> <span class=n>reuters</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>train_data</span><span class=p>,</span> <span class=n>train_label</span><span class=p>),</span> <span class=p>(</span><span class=n>test_data</span><span class=p>,</span> <span class=n>test_label</span><span class=p>)</span> <span class=o>=</span> <span class=n>reuters</span><span class=o>.</span><span class=n>load_data</span><span class=p>(</span><span class=n>num_words</span><span class=o>=</span><span class=mi>10000</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=将向量还原为原始新闻非必须>将向量还原为原始新闻（非必须）</h3><p>执行这一步只是为了更直观的了解别人是怎么处理数据的。</p><p>这里同样会下载数据。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>word_index</span> <span class=o>=</span> <span class=n>reuters</span><span class=o>.</span><span class=n>get_word_index</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>reverse_word_index</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>([(</span><span class=n>value</span><span class=p>,</span> <span class=n>key</span><span class=p>)</span> <span class=k>for</span> <span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>)</span> <span class=ow>in</span> <span class=n>word_index</span><span class=o>.</span><span class=n>items</span><span class=p>()])</span>
</span></span><span class=line><span class=cl><span class=n>decoded_newswire</span> <span class=o>=</span> <span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>([</span><span class=n>reverse_word_index</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>i</span><span class=o>-</span><span class=mi>3</span><span class=p>,</span> <span class=s1>&#39;?&#39;</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>train_data</span><span class=p>[</span><span class=mi>0</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>decoded_newswire</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=将数据向量化>将数据向量化</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 数据向量化</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>vectorize_sequences</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>dimension</span><span class=o>=</span><span class=mi>10000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>results</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=nb>len</span><span class=p>(</span><span class=n>sequences</span><span class=p>),</span> <span class=n>dimension</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>sequence</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>sequences</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>results</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>sequence</span><span class=p>]</span> <span class=o>=</span> <span class=mf>1.</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>results</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x_train</span> <span class=o>=</span> <span class=n>vectorize_sequences</span><span class=p>(</span><span class=n>train_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x_test</span> <span class=o>=</span> <span class=n>vectorize_sequences</span><span class=p>(</span><span class=n>test_data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=使用-one-hot-编码将标签向量化>使用 one-hot 编码将标签向量化</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># one-hot编码方法实现</span>
</span></span><span class=line><span class=cl><span class=c1># def to_one_hot(labels, dimension=10000):</span>
</span></span><span class=line><span class=cl><span class=c1>#     results = np.zeros((len(labels), dimension))</span>
</span></span><span class=line><span class=cl><span class=c1>#     for i, label in enumerate(labels):</span>
</span></span><span class=line><span class=cl><span class=c1>#         results[i, label] = 1.</span>
</span></span><span class=line><span class=cl><span class=c1>#     return results</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用keras内置方法将标签向量化</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.utils.np_utils</span> <span class=kn>import</span> <span class=n>to_categorical</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>one_hot_train_labels</span> <span class=o>=</span> <span class=n>to_categorical</span><span class=p>(</span><span class=n>train_label</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>one_hot_test_labels</span> <span class=o>=</span> <span class=n>to_categorical</span><span class=p>(</span><span class=n>test_label</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=构建模型>构建模型</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 定义模型</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras</span> <span class=kn>import</span> <span class=n>models</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras</span> <span class=kn>import</span> <span class=n>layers</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>Q: 为什么此处输入单元数要使用64，为什么不使用电影评论分类时使用的16？
</span></span></span><span class=line><span class=cl><span class=s2>A：16维空间对于这个例子来说太小了，无法学会区分46个不同的类别。
</span></span></span><span class=line><span class=cl><span class=s2>   这种维度较小的层可能成为信息瓶颈，永久地丢失相关信息。
</span></span></span><span class=line><span class=cl><span class=s2>   如果是三分类，四分类问题你依然可以使用16个隐藏单元
</span></span></span><span class=line><span class=cl><span class=s2>Q：我能不能设置为640个单元？
</span></span></span><span class=line><span class=cl><span class=s2>A：单元数不是越大越好，网络容量越大，网络就越容易记住训练过的数据。
</span></span></span><span class=line><span class=cl><span class=s2>   网络会在训练过的数据上表现优异，但是在没有见过的数据上的表现则不容乐观。
</span></span></span><span class=line><span class=cl><span class=s2>   因此单元数不是越大越好，需要在欠拟合与过拟合之间找到一个平衡点。
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>10000</span><span class=p>,</span> <span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>46</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 编译模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;rmsprop&#39;</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s1>&#39;categorical_crossentropy&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=验证模型>验证模型</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 留出验证集</span>
</span></span><span class=line><span class=cl><span class=n>x_val</span> <span class=o>=</span> <span class=n>x_train</span><span class=p>[:</span><span class=mi>1000</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>partial_x_train</span> <span class=o>=</span> <span class=n>x_train</span><span class=p>[</span><span class=mi>1000</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>y_val</span><span class=o>=</span><span class=n>one_hot_train_labels</span><span class=p>[:</span><span class=mi>1000</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>partial_y_train</span> <span class=o>=</span> <span class=n>one_hot_train_labels</span><span class=p>[</span><span class=mi>1000</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练模型</span>
</span></span><span class=line><span class=cl><span class=n>history</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>partial_x_train</span><span class=p>,</span> <span class=n>partial_y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>x_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=绘制训练情况图表>绘制训练情况图表</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 绘制训练损失和验证损失</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;loss&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>val_loss</span> <span class=o>=</span> <span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;val_loss&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>epochs</span><span class=p>,</span> <span class=n>loss</span><span class=p>,</span> <span class=s1>&#39;bo&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>epochs</span><span class=p>,</span> <span class=n>val_loss</span><span class=p>,</span> <span class=s1>&#39;b&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Validation loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Training and validation loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epochs&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 绘制训练精度和验证精度</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>clf</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>acc</span> <span class=o>=</span> <span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>val_acc</span> <span class=o>=</span> <span class=n>history</span><span class=o>.</span><span class=n>history</span><span class=p>[</span><span class=s1>&#39;val_accuracy&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>epochs</span><span class=p>,</span> <span class=n>acc</span><span class=p>,</span> <span class=s1>&#39;bo&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training acc&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>epochs</span><span class=p>,</span> <span class=n>val_acc</span><span class=p>,</span> <span class=s1>&#39;b&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Validation acc&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Training and validation accuracy&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epochs&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Accuracy&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p><img src=https://raw.githubusercontent.com/wlynxg/pic/main/2025/06/01/20250601-165441.png loading=lazy alt=image.png></p><p><img src=https://raw.githubusercontent.com/wlynxg/pic/main/2025/06/01/20250601-165451.png loading=lazy alt=image.png></p><h3 id=重新训练模型>重新训练模型</h3><p>观察图表，发现模型在第十轮附近出现过拟合现象。那么我们重新训练模型就训练九轮就行（可以尝试其它的）。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 重新训练模型</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>10000</span><span class=p>,</span> <span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>46</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;rmsprop&#39;</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s1>&#39;categorical_crossentropy&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>history</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>partial_x_train</span><span class=p>,</span> <span class=n>partial_y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>9</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>x_val</span><span class=p>,</span> <span class=n>y_val</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># 观察在测试集上表现</span>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>evaluate</span><span class=p>(</span><span class=n>x_test</span><span class=p>,</span> <span class=n>one_hot_test_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>results</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># [0.9868815943054715, 0.7862867116928101]</span>
</span></span><span class=line><span class=cl><span class=c1># 80%左右的精度</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 采取随机预测的方式</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>copy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>test_labels_copy</span> <span class=o>=</span> <span class=n>copy</span><span class=o>.</span><span class=n>copy</span><span class=p>(</span><span class=n>test_label</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>test_labels_copy</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hits_array</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>test_label</span><span class=p>)</span> <span class=o>==</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>test_labels_copy</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>float</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>hits_array</span><span class=p>))</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>test_label</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># 0.18788958147818344</span>
</span></span><span class=line><span class=cl><span class=c1># 20%的精度，可以看出模型的预测效果好得多</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=绘制图表观察重新训练的模型各项指标-->绘制图表观察重新训练的模型各项指标 -</h3><p><img src=https://raw.githubusercontent.com/wlynxg/pic/main/2025/06/01/20250601-165503.png loading=lazy alt=image.png>
<img src=https://raw.githubusercontent.com/wlynxg/pic/main/2025/06/01/20250601-165553.png loading=lazy alt=image.png></p><h2 id=小结>小结</h2><ul><li>如果要对N个类别的数据点进行分类，<strong>网络的最后一层应该是大小为N的Dense层</strong>。</li><li>对于单标签、多分类问题，<strong>网络的最后一层应该使用 softmax 激活</strong>，这样可以输出在N个输出类别上的概率分布。</li><li>这种问题的损失函数几乎总是应该使用<strong>分类交叉熵</strong>。它将网络输出的概率分布与目标的真实分布之间的距离最小化。</li><li>如果你需要将数据划分到许多类别中，应该<strong>避免使用太小的中间层</strong>，以免在网络中造成信息瓶颈。</li><li>处理多分类问题的标签有两种方法。<ul><li>通过分类编码（也叫one-hot编码）对标签进行编码，然后使用categorical_crossentropy 作为损失函数。</li><li>将标签编码为整数，然后使用 sparse_categorical_crossentropy 损失函数。</li></ul></li></ul></section><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0}),document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll("pre code.language-mermaid");e.forEach(function(e){var n,t=document.createElement("div");t.className="mermaid",t.innerHTML=e.textContent,n=e.parentElement,n.parentNode.replaceChild(t,n)})})</script><footer class=article-footer><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/keras-%E4%B8%AD-model.evaluate-%E5%92%8C-model.predict-%E7%9A%84%E5%8C%BA%E5%88%AB/><div class=article-details><h2 class=article-title>Keras 中 model.evaluate 和 model.predict 的区别</h2></div></a></article><article><a href=/blog/p/numpy-%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E4%B8%8E%E6%96%B9%E6%B3%95/><div class=article-details><h2 class=article-title>numpy 常用函数与方法</h2></div></a></article><article><a href=/blog/p/pandas-%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E4%B8%8E%E6%96%B9%E6%B3%95/><div class=article-details><h2 class=article-title>pandas 常用函数与方法</h2></div></a></article><article><a href=/blog/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%93%E6%9C%89%E5%90%8D%E8%AF%8D%E5%BD%92%E7%BA%B3/><div class=article-details><h2 class=article-title>机器学习专有名词归纳</h2></div></a></article><article><a href=/blog/p/%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA%E5%88%86%E7%B1%BB%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/><div class=article-details><h2 class=article-title>电影评论分类：二分类问题</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Wlynxg's Blog</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>